{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Volumes Aggregation from SynthSeg Outputs\n",
    "\n",
    "## Obiettivo\n",
    "Aggregare i volumi cerebrali e i QC scores da SynthSeg per tutti i dataset disponibili.\n",
    "\n",
    "## Dataset Processati\n",
    "1. **OASIS-2**: 150 soggetti - Cross-sectional MRI data\n",
    "2. **OASIS-3**: 1,376 soggetti - Longitudinal neuroimaging dataset\n",
    "3. **ADNI**: 520 soggetti - Alzheimer's Disease Neuroimaging Initiative\n",
    "4. **IXI**: 346 soggetti - Information eXtraction from Images\n",
    "5. **PPMI**: 160 soggetti - Parkinson's Progression Markers Initiative\n",
    "6. **SRPBS**: 1,410 soggetti - Southwest University Adult Lifespan Dataset\n",
    "\n",
    "\n",
    "## Dataset da processare\n",
    "1. **OASIS-1** :  Problemi con la conversione, orientamento delle immagini fallisce\n",
    "2. **AABC**  : Problemi con il download dovrebbero risolvere entro il 30/11\n",
    "\n",
    "## Output\n",
    "Per ogni dataset viene creato un CSV in `/data/volumes/` contenente:\n",
    "- **101 metriche volumetriche**: strutture sottocorticali, regioni corticali (Desikan-Killiany), misure globali\n",
    "- **8 QC scores**: quality control per diverse regioni cerebrali\n",
    "- **subject_id**: identificativo del soggetto\n",
    "\n",
    "## Struttura SynthSeg\n",
    "```\n",
    "dataset/derivatives/synthseg/\n",
    "└── {subject_id}/\n",
    "    ├── volumes.csv          # Volumi regionali (50+ regioni)\n",
    "    ├── qc_scores.csv        # Quality control scores (8 metriche)\n",
    "    ├── segmentation.nii.gz  # Maschera di segmentazione\n",
    "    └── resampled.nii.gz     # T1w resampleata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Paths\n",
    "\n",
    "Percorsi alle cartelle `derivatives/synthseg/` per ciascun dataset.\n",
    "\n",
    "**Note**:\n",
    "- ADNI: path corretto è `/mnt/db_ext/ADNI_DB/NIFTI_CN/` (non ancora riorganizzato in BIDS derivatives)\n",
    "- OASIS-1: non incluso (problemi di conversione e orientamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to SynthSeg derivatives folders\n",
    "oasis1_path = \"/mnt/db_ext/RAW/oasis/OASIS1_BIDS/\"  # Not yet processed\n",
    "oasis2_path = \"/mnt/db_ext/RAW/oasis/OASIS2_BIDS/derivatives/synthseg/\"\n",
    "oasis3_path = \"/mnt/db_ext/RAW/oasis/OASIS3_BIDS/derivatives/synthseg/\"\n",
    "adni_path = \"/mnt/db_ext/ADNI_DB/NIFTI_CN/\"  # Original location with processed/freesurfer8/ structure\n",
    "ixi_path = \"/mnt/db_ext/RAW/IXI/derivatives/synthseg/\"\n",
    "ppmi_path = \"/mnt/db_ext/RAW/PPMI/nifti/derivatives/synthseg/\"\n",
    "srpb_path = \"/mnt/db_ext/RAW/SRPBS_OPEN/SRPBS_BIDS/derivatives/synthseg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function: Find Volumes and QC Files\n",
    "\n",
    "Questa funzione:\n",
    "1. Scandisce ricorsivamente la cartella del dataset\n",
    "2. Cerca directory contenenti sia `volumes.csv` che `qc_scores.csv`\n",
    "3. Estrae il `subject_id` dal nome della directory\n",
    "4. Restituisce un DataFrame con i path ai file per ogni soggetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_volumes_and_qc(path):\n",
    "    \"\"\"\n",
    "    Find all volumes.csv and qc_scores.csv files in a dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to dataset derivatives/synthseg folder\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [subject_id, volumes, qc]\n",
    "    \"\"\"\n",
    "    volumes_list = []\n",
    "    \n",
    "    # Walk through all subdirectories\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Check if both required files exist\n",
    "        if 'volumes.csv' in files and 'qc_scores.csv' in files:\n",
    "            subject_id = os.path.basename(root)\n",
    "            volumes_list.append({\n",
    "                'subject_id': subject_id,\n",
    "                'volumes': os.path.join(root, 'volumes.csv'),\n",
    "                'qc': os.path.join(root, 'qc_scores.csv')\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(volumes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function: Aggregate Volumes and QC\n",
    "\n",
    "Questa funzione:\n",
    "1. Carica i file `volumes.csv` e `qc_scores.csv` per ogni soggetto\n",
    "2. Aggiunge prefissi `vol_` e `qc_` alle colonne per distinguerle\n",
    "3. Merge dei due DataFrame sulla colonna subject\n",
    "4. Verifica che i subject_id corrispondano\n",
    "5. Aggiunge il subject_id come colonna\n",
    "6. Concatena tutti i soggetti in un unico DataFrame\n",
    "\n",
    "**Output**: Un DataFrame con una riga per soggetto e ~110 colonne (101 volumi + 8 QC + subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_volumes_and_qc(volumes_df):\n",
    "    \"\"\"\n",
    "    Aggregate volumes and QC scores for all subjects.\n",
    "    \n",
    "    Args:\n",
    "        volumes_df (pd.DataFrame): DataFrame from find_volumes_and_qc()\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated data with columns:\n",
    "                      - vol_* (101 volume metrics)\n",
    "                      - qc_* (8 QC scores)\n",
    "                      - subject_id\n",
    "    \"\"\"\n",
    "    aggregated_data = []\n",
    "    \n",
    "    for idx, row in volumes_df.iterrows():\n",
    "        # Load volumes and QC files\n",
    "        vol_df = pd.read_csv(row['volumes'])\n",
    "        qc_df = pd.read_csv(row['qc'])\n",
    "        \n",
    "        # Add prefixes to distinguish column types\n",
    "        vol_df = vol_df.add_prefix('vol_')\n",
    "        qc_df = qc_df.add_prefix('qc_')\n",
    "        \n",
    "        subject_id = row['subject_id']\n",
    "        \n",
    "        # Merge volumes and QC on subject column\n",
    "        merged_df = pd.merge(vol_df, qc_df, left_on='vol_subject', right_on='qc_subject')\n",
    "        \n",
    "        # Verify subject IDs match\n",
    "        if not merged_df['vol_subject'].equals(merged_df['qc_subject']):\n",
    "            raise ValueError(f\"Mismatch between vol_subject and qc_subject for subject {subject_id}\")\n",
    "        \n",
    "        # Add subject_id as a column (not as index yet)\n",
    "        merged_df['subject_id'] = subject_id\n",
    "        \n",
    "        # Drop redundant subject columns and set subject_id as index\n",
    "        merged_df = merged_df.drop(columns=['vol_subject', 'qc_subject']).set_index('subject_id')\n",
    "        \n",
    "        aggregated_data.append(merged_df)\n",
    "    \n",
    "    # Concatenate all subjects\n",
    "    return pd.concat(aggregated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process All Datasets\n",
    "\n",
    "### 5.1 Find volumes and QC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find volumes and QC files for each dataset\n",
    "print(\"Finding volumes and QC files...\\n\")\n",
    "\n",
    "oasis2_volumes = find_volumes_and_qc(oasis2_path)\n",
    "print(f\"OASIS-2: {len(oasis2_volumes)} subjects\")\n",
    "\n",
    "oasis3_volumes = find_volumes_and_qc(oasis3_path)\n",
    "print(f\"OASIS-3: {len(oasis3_volumes)} subjects\")\n",
    "\n",
    "srpb_volumes = find_volumes_and_qc(srpb_path)\n",
    "print(f\"SRPBS: {len(srpb_volumes)} subjects\")\n",
    "\n",
    "adni_volumes = find_volumes_and_qc(adni_path)\n",
    "print(f\"ADNI: {len(adni_volumes)} subjects\")\n",
    "\n",
    "ixi_volumes = find_volumes_and_qc(ixi_path)\n",
    "print(f\"IXI: {len(ixi_volumes)} subjects\")\n",
    "\n",
    "ppmi_volumes = find_volumes_and_qc(ppmi_path)\n",
    "print(f\"PPMI: {len(ppmi_volumes)} subjects\")\n",
    "\n",
    "total_subjects = (len(oasis2_volumes) + len(oasis3_volumes) + len(srpb_volumes) + \n",
    "                  len(adni_volumes) + len(ixi_volumes) + len(ppmi_volumes))\n",
    "print(f\"\\nTotal subjects with SynthSeg: {total_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Aggregate volumes and QC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate volumes and QC for each dataset\n",
    "print(\"\\nAggregating volumes and QC scores...\\n\")\n",
    "\n",
    "oasis2_aggregated = aggregate_volumes_and_qc(oasis2_volumes)\n",
    "print(f\"OASIS-2: {oasis2_aggregated.shape}\")\n",
    "\n",
    "oasis3_aggregated = aggregate_volumes_and_qc(oasis3_volumes)\n",
    "print(f\"OASIS-3: {oasis3_aggregated.shape}\")\n",
    "\n",
    "adni_aggregated = aggregate_volumes_and_qc(adni_volumes)\n",
    "print(f\"ADNI: {adni_aggregated.shape}\")\n",
    "\n",
    "ixi_aggregated = aggregate_volumes_and_qc(ixi_volumes)\n",
    "print(f\"IXI: {ixi_aggregated.shape}\")\n",
    "\n",
    "ppmi_aggregated = aggregate_volumes_and_qc(ppmi_volumes)\n",
    "print(f\"PPMI: {ppmi_aggregated.shape}\")\n",
    "\n",
    "srpb_aggregated = aggregate_volumes_and_qc(srpb_volumes)\n",
    "print(f\"SRPBS: {srpb_aggregated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Inspect column structure\n",
    "\n",
    "Verifichiamo la struttura delle colonne per un dataset di esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column structure for ADNI as example\n",
    "print(\"\\nColumn structure (ADNI example):\")\n",
    "print(f\"Total columns: {len(adni_aggregated.columns)}\\n\")\n",
    "\n",
    "# Volume columns\n",
    "vol_cols = [col for col in adni_aggregated.columns if col.startswith('vol_')]\n",
    "print(f\"Volume metrics: {len(vol_cols)}\")\n",
    "print(\"First 10 volume columns:\")\n",
    "for col in vol_cols[:10]:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# QC columns\n",
    "qc_cols = [col for col in adni_aggregated.columns if col.startswith('qc_')]\n",
    "print(f\"\\nQC scores: {len(qc_cols)}\")\n",
    "for col in qc_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Aggregated Data\n",
    "\n",
    "Salviamo un CSV per dataset nella cartella `/data/volumes/`.\n",
    "\n",
    "**IMPORTANTE**: I file vengono salvati con `index=False`, ma il subject_id è stato aggiunto come colonna\n",
    "prima di impostarlo come index, quindi la colonna `subject_id` sarà l'ultima colonna nel CSV finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"/home/mario/Repository/Normal_Alzeihmer/data/volumes/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving aggregated data to {output_dir}\\n\")\n",
    "\n",
    "# Save each dataset\n",
    "oasis2_aggregated.to_csv(output_dir + \"oasis2.csv\", index=False)\n",
    "print(f\"✓ Saved: oasis2.csv ({len(oasis2_aggregated)} subjects)\")\n",
    "\n",
    "oasis3_aggregated.to_csv(output_dir + \"oasis3.csv\", index=False)\n",
    "print(f\"✓ Saved: oasis3.csv ({len(oasis3_aggregated)} subjects)\")\n",
    "\n",
    "adni_aggregated.to_csv(output_dir + \"adni.csv\", index=False)\n",
    "print(f\"✓ Saved: adni.csv ({len(adni_aggregated)} subjects)\")\n",
    "\n",
    "ixi_aggregated.to_csv(output_dir + \"ixi.csv\", index=False)\n",
    "print(f\"✓ Saved: ixi.csv ({len(ixi_aggregated)} subjects)\")\n",
    "\n",
    "ppmi_aggregated.to_csv(output_dir + \"ppmi.csv\", index=False)\n",
    "print(f\"✓ Saved: ppmi.csv ({len(ppmi_aggregated)} subjects)\")\n",
    "\n",
    "srpb_aggregated.to_csv(output_dir + \"srpb.csv\", index=False)\n",
    "print(f\"✓ Saved: srpb.csv ({len(srpb_aggregated)} subjects)\")\n",
    "\n",
    "print(f\"\\n✓ Total files saved: 6\")\n",
    "print(f\"✓ Total subjects: {total_subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics\n",
    "\n",
    "Calcoliamo alcune statistiche di base per verificare i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for a key region (hippocampus)\n",
    "print(\"\\nSummary statistics for hippocampal volume (left + right):\\n\")\n",
    "\n",
    "datasets = {\n",
    "    'OASIS-2': oasis2_aggregated,\n",
    "    'OASIS-3': oasis3_aggregated,\n",
    "    'ADNI': adni_aggregated,\n",
    "    'IXI': ixi_aggregated,\n",
    "    'PPMI': ppmi_aggregated,\n",
    "    'SRPBS': srpb_aggregated\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    left_hipp = df['vol_left hippocampus']\n",
    "    right_hipp = df['vol_right hippocampus']\n",
    "    total_hipp = left_hipp + right_hipp\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean: {total_hipp.mean():.1f} mm³\")\n",
    "    print(f\"  Std:  {total_hipp.std():.1f} mm³\")\n",
    "    print(f\"  Range: [{total_hipp.min():.1f}, {total_hipp.max():.1f}] mm³\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Files\n",
    "\n",
    "I seguenti file CSV sono stati creati in `/data/volumes/`:\n",
    "\n",
    "1. **oasis2.csv** - 150 soggetti × 110 colonne\n",
    "2. **oasis3.csv** - 1,376 soggetti × 110 colonne\n",
    "3. **adni.csv** - 520 soggetti × 110 colonne\n",
    "4. **ixi.csv** - 346 soggetti × 110 colonne\n",
    "5. **ppmi.csv** - 160 soggetti × 110 colonne\n",
    "6. **srpb.csv** - 1,410 soggetti × 110 colonne\n",
    "\n",
    "**Totale: 3,962 soggetti**\n",
    "\n",
    "## Struttura delle Colonne\n",
    "\n",
    "Ogni CSV contiene:\n",
    "- **101 colonne vol_***: Volumi cerebrali in mm³\n",
    "  - Subcortical: hippocampus, amygdala, thalamus, caudate, putamen, pallidum, accumbens\n",
    "  - Cortical: 68 regioni (Desikan-Killiany atlas)\n",
    "  - Global: TIV, white matter, cortex, cerebellum, ventricles, CSF, brainstem\n",
    "- **8 colonne qc_***: Quality control scores (0-1, higher is better)\n",
    "  - qc_general white matter\n",
    "  - qc_general grey matter\n",
    "  - qc_general csf\n",
    "  - qc_cerebellum\n",
    "  - qc_brainstem\n",
    "  - qc_thalamus\n",
    "  - qc_putamen+pallidum\n",
    "  - qc_hippocampus+amygdala\n",
    "- **1 colonna subject_id**: Identificativo soggetto (ultima colonna)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
